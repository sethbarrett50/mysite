<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta name="robots" content="index, follow">

    <meta name="description"
        content="This blog post provides an in-depth introduction to machine learning in Julia using the Flux.jl package, a powerful framework for creating and training deep learning models. It dives into the essentials of setting up the package, building simple models, training them using a synthetic dataset, and monitoring their progress. The post also provides instructions for evaluating the performance of a trained model and saving it for future use. Part of an ongoing series on Julia, this post is an invaluable resource for those interested in harnessing the power of Julia for deep learning and other machine learning applications.">
    <meta name="keywords"
        content="Seth, Seth Barrett, Augusta, Augusta University, AU, SCCS, School of Computer and Cyber Science, Barrett, Dorai, PhD, Computer science, Cybersecurity, Digital forensics, Machine learning, Bioinformatics, Internet of things, Networking, Vulnerability analysis, Research, Education, PhD degree, Cyber sciences, Data science, Cybersecurity research, Digital forensics research, Machine learning research, Bioinformatics research, Internet of things research, Networking research, Vulnerability analysis research, Computer science research, Cybersecurity education, Digital forensics education, Machine learning education, Bioinformatics education, Internet of things education, Networking education, Vulnerability analysis education, Computer science education, PhD program, Cybersecurity program, Digital forensics program, Machine learning program, Bioinformatics program, Internet of things program, Networking program, Vulnerability analysis program, Computer science program, Graduate studies, Cybersecurity graduate studies, Digital forensics graduate studies, Machine learning graduate studies, Bioinformatics graduate studies, Internet of things graduate studies, Networking graduate studies, Vulnerability analysis graduate studies, Computer science graduate studies, PhD studies, Cybersecurity studies, Digital forensics studies, Machine learning studies, Bioinformatics studies, Internet of things studies, Networking studies, Vulnerability analysis studies, Computer science studies, Academic research, Cybersecurity research, Digital forensics research, Machine learning research, Bioinformatics research, Internet of things research, Networking research, Vulnerability analysis research, Computer science research, Professional development, Cybersecurity professional development, Digital forensics professional development, Machine learning professional development, Bioinformatics professional development, Internet of things professional development, Networking professional development, Vulnerability analysis professional development, Computer science professional development, Continuing education, Cybersecurity continuing education, Digital forensics continuing education, Machine learning continuing education, Bioinformatics continuing education, Internet of things continuing education, Networking continuing education, Vulnerability analysis continuing education, Computer science continuing education, Career advancement, Cybersecurity career advancement, Digital forensics career advancement, Machine learning career advancement, Bioinformatics career advancement, Internet of things career advancement, Networking career advancement, Vulnerability analysis career advancement, Computer science career advancement, Professional portfolio, Cybersecurity portfolio, Digital forensics portfolio, Machine learning portfolio, Bioinformatics portfolio, Internet of things portfolio, Networking portfolio">
    <meta name="author" content="Seth Barrett">


    <meta property="og:site_name" content="Seth Barrett - Home">
    <meta name="twitter:domain" property="twitter:domain" content="sethbarrett.xyz">
    <meta name="og:title" property="og:title" content="Seth Barrett - Home">
    <meta property="og:description"
        content="This blog post provides an in-depth introduction to machine learning in Julia using the Flux.jl package, a powerful framework for creating and training deep learning models. It dives into the essentials of setting up the package, building simple models, training them using a synthetic dataset, and monitoring their progress. The post also provides instructions for evaluating the performance of a trained model and saving it for future use. Part of an ongoing series on Julia, this post is an invaluable resource for those interested in harnessing the power of Julia for deep learning and other machine learning applications.">
    <meta name="twitter:description" property="twitter:description"
        content="This blog post provides an in-depth introduction to machine learning in Julia using the Flux.jl package, a powerful framework for creating and training deep learning models. It dives into the essentials of setting up the package, building simple models, training them using a synthetic dataset, and monitoring their progress. The post also provides instructions for evaluating the performance of a trained model and saving it for future use. Part of an ongoing series on Julia, this post is an invaluable resource for those interested in harnessing the power of Julia for deep learning and other machine learning applications.">
    <meta name="og:image" content="https://sethbarrett.xyz/photos/me.webp">


    <meta property="twitter:card" content="https://sethbarrett.xyz/photos/me.webp">
    <meta name="twitter:image:src" property="twitter:image:src" content="https://sethbarrett.xyz/photos/me.webp">
    <meta name="twitter:image" property="twitter:image" content="https://sethbarrett.xyz/photos/me.webp">
    <meta name="og:image:alt" property="og:image:alt" content="Me">

    <meta property="og:url" content="sethbarrett.xyz">
    <meta property="og:type" content="website">
    <title>Daily Blog Post: June 27th, 2023</title>


    <!-- Update CSS Versions when changed -->
    <link rel="stylesheet" href="../selfie.css?v=1.0.8" />
    <link rel="stylesheet" href="../selfie_tablet.css?v=1.0.8" media="only screen and (max-width: 1000px)" />
    <link rel="stylesheet" href="../selfie_mobile.css?v=1.0.8" media="only screen and (max-width: 600px)" />
</head>

<body>
    <div id="wrapper">
        <header>
            <h1>Seth Barrett</h1>
            <h2>Daily Blog Post: June 27th, 2023</h2>
        </header>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../experience.html">Experience</a></li>
                <li><a href="../education.html">Education</a></li> 
                <li><a href="../blog.html">Blog</a></li>
                <li><a href="../contact.html">Contact Me</a></li>
            </ul>
        </nav>
        <main>
            <div id="blog">
                
                <div class="blog">
                    <img src="photos/06_27_23.webp" alt="go" width="250" height="445" />
                    <h4>June 27th, 2023</h4>
                    <div class='title'>Machine Learning with Flux.jl: An Introduction to Julia's Deep Learning Framework</div>

                    <p>
                        Welcome back to our series on Julia, the high-performance programming language designed for scientific computing. We have covered various aspects of the language, including setting up a coding environment, syntax and unique features, data science, machine learning techniques, optimization strategies, working with databases, building web applications, web scraping, data visualization, time series forecasting, deep learning, mathematical optimization, scientific applications, advanced numerical computing, optimization and root-finding with NLsolve.jl, statistical modeling with GLM.jl, and numerical integration with QuadGK.jl. In this post, we will focus on machine learning in Julia, introducing the Flux.jl package and demonstrating how to create and train deep learning models using this powerful and flexible framework.
                    </p>
                    <h5>Overview of Machine Learning Packages in Julia</h5>
                    <p>
                        There are several machine learning packages available in Julia, including:
                    </p>
                    <ol>
                        <li>
                            <b>Flux.jl</b>: A powerful and flexible deep learning framework designed from the ground up for Julia, with support for automatic differentiation, GPU acceleration, and various model architectures.
                        </li>
                        <li>
                            <b>MLJ.jl</b>: A comprehensive toolbox for machine learning, offering a unified interface to various algorithms, tools for composing and tuning models, and support for data manipulation and preprocessing.
                        </li>
                        <li>
                            <b>ScikitLearn.jl</b>: A wrapper around the popular Python library scikit-learn, providing a familiar interface for users coming from the Python ecosystem.
                        </li>
                    </ol>
                    <p>
                        In this post, we will focus on Flux.jl, which provides an intuitive and expressive way to create and train deep learning models in Julia.
                    </p>
                    <h5>Getting Started with Flux.jl</h5>
                    <p>To get started with Flux.jl, you first need to install the package:</p>
<p><pre>import Pkg
Pkg.add("Flux")</pre></p>
                    <p>
                        Now, you can use Flux to create simple deep learning models:
                    </p>
<p><pre>using Flux

# Define a simple multi-layer perceptron with one hidden layer
model = Chain(
    Dense(10, 5, relu),
    Dense(5, 2),
    softmax
)

# Print the model architecture
println(model)</pre></p>
                <p>
                    In this example, we define a simple multi-layer perceptron with one hidden layer containing five neurons and an output layer with two neurons. We use the <code>Chain</code> function to create a sequential model, and the <code>Dense</code> function to create fully connected layers. The activation functions are specified using the <code>relu</code> and <code>softmax</code> functions.
                </p>      
                <h5>Training a Deep Learning Model</h5> 
                <p>To train a deep learning model with Flux, you need to define a loss function, an optimizer, and a dataset:</p>
<p><pre>using Flux, Random

# Generate a synthetic dataset
X = rand(10, 100)
Y = onehotbatch(rand(1:2, 100), 1:2)

# Define the loss function
loss(x, y) = Flux.Losses.crossentropy(model(x), y)

# Define the optimizer
optimizer = ADAM(0.001)

# Train the model
Flux.train!(loss, params(model), [(X, Y)], optimizer)</pre></p>
                <p>
                    In this example, we generate a synthetic dataset with 10-dimensional input features and two output classes. We define the loss function using the <code>crossentropy</code> function from Flux's <code>Losses</code> submodule. The optimizer is defined using the <code>ADAM</code> function with a learning rate of 0.001. The <code>train!</code> function is used to update the model's parameters during training.
                </p>
                <h5>Monitoring Training Progress</h5>
                <p>To monitor the progress of training, you can use Flux's built-in callback system:</p>
<p><pre>using Flux

# Define the callback function
callback() = @info("Loss: $(loss(X, Y))")

# Train the model with the callback
Flux.train!(loss, params(model), [(X, Y)], optimizer, cb=throttle(callback, 10))</pre></p>
                <p>
                    In this example, we define a callback function that prints the current value of the loss function. The `@info` macro is used to display the information in a formatted manner. The `throttle` function is used to limit the frequency of the callback execution to once every 10 seconds.
                </p>
                <h5>Evaluating and Saving the Trained Model</h5>
                <p>
                    Once the model is trained, you can evaluate its performance on a test dataset and save the trained model for future use:
                </p>
<p><pre>using Flux, BSON

# Generate a test dataset
X_test = rand(10, 50)
Y_test = onehotbatch(rand(1:2, 50), 1:2)

# Compute the accuracy of the model on the test dataset
accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))
println("Test accuracy: $(accuracy(X_test, Y_test))")

# Save the trained model to a file
BSON.@save "trained_model.bson" model</pre></p>
                <p>
                    In this example, we generate a test dataset and compute the accuracy of the trained model using the <code>accuracy</code> function. The <code>onecold</code> function is used to convert the model's output probabilities into class labels. Finally, we save the trained model to a BSON file using the BSON package and the <code>@save</code> macro.
                </p>
                <h5>Conclusion</h5>
                <p>
                    In this post, we introduced machine learning in Julia using the Flux.jl package. We demonstrated how to create and train deep learning models using an intuitive and expressive syntax, with support for automatic differentiation, GPU acceleration, and various model architectures. Flux.jl provides a powerful and flexible framework for various applications in machine learning, data analysis, and other fields.
                </p>
                <p>
                    As we continue our series on Julia, stay tuned for more posts covering a wide range of topics, from parallel processing and distributed computing to high-performance computing and scientific applications. We will explore various packages and techniques, equipping you with the knowledge and skills required to tackle complex problems in your domain.
                </p>
                <p>
                    In upcoming posts, we will delve deeper into advanced numerical computing, discussing topics such as data manipulation with DataFrames.jl, optimization with JuMP.jl, and graph algorithms with LightGraphs.jl. These topics will further enhance your understanding of Julia and its capabilities, enabling you to become a proficient Julia programmer.
                </p>
                <p>
                    Keep learning, and happy coding!
                </p>
                    
                    
                    <button><a href="./06_28_2023.html">Next Post in Series</a></button>
                    <button><a href="./06_26_2023.html">Previous Post in Series</a></button>

                    
                    <!-- Desc: This blog post provides an in-depth introduction to machine learning in Julia using the Flux.jl package, a powerful framework for creating and training deep learning models. It dives into the essentials of setting up the package, building simple models, training them using a synthetic dataset, and monitoring their progress. The post also provides instructions for evaluating the performance of a trained model and saving it for future use. Part of an ongoing series on Julia, this post is an invaluable resource for those interested in harnessing the power of Julia for deep learning and other machine learning applications.  -->
                        
                </div>
            </div>
        </main>
        <footer>
            <!-- Copyright &copy; [2023], [Seth Barrett]
            <br /> -->
            <b>
                Email:
            </b>
            <a href="mailto:sebarrett@augusta.edu">sebarrett@augusta.edu</a>
            <br />
            <b>
                Blog RSS Feed:
            </b>
            <a href="sethbarrett.xyz/blogposts/rss.xml">sethbarrett.xyz/blogposts/rss.xml</a>
            <br />
            <b>
                Linkedin Profile:
            </b>
            <a href="https://www.linkedin.com/in/975833b14567812q">LinkedIn</a>
            <br />
            <b>
                GitHub:
            </b>
            <a href="https://github.com/sethbarrett50">GitHub</a>
            <br />
            <b>
                Privacy Policy:
            </b>
            <a href="privacy.html">Privacy Policy</a>
        </footer>
    </div>
</body>
</html>